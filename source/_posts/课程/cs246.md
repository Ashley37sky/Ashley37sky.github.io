---
title: cs246-spark
date: 2024-01-25 10:58:56
tags: cs246
categories: cs246
---

# spark
## 概念
  <!--more-->
+ SparkContext & SparkSession
  + SparkContext
    + 早期入口点，用于与Spark集群进行连接和通信
    + 创建并操作RDD（Resilient Distributed Dataset）的主要入口点。RDD是Spark的基本数据抽象，用于分布式数据处理
  + SparkSession：
    + 封装了SparkContext，并提供了对DataFrame和SQL操作的支持，同时还支持Spark Streaming、MLlib和GraphX等。

## dataframe
+ 创建SparkSession

    ``` python
    from pyspark.sql import SparkSession

    # 创建SparkSession
    spark = SparkSession.builder \
        .appName("MySparkApp") \  # 设置应用程序名称
        .config("spark.some.config.option", "config-value") \  # 配置Spark参数
        .getOrCreate()
    ```
+ 读取数据
  ```python
    # 读取 / 创建数据
    # inferSchema=True, 自动推断列的数据类型, 否则全部视为字符串
    df = spark.read.csv("data.csv", header=True, inferSchema=True)

    # 创建DataFrame
    data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
    df = spark.createDataFrame(data, ["Name", "Age"])

    # 显示数据
    df.show() # top 20 in table
    df.printSchema() # structure
    df.take(3)
    df.count()
  ``` 
+ 持久化
  ``` python
  df.cache() # 内存

  # 指定level
  df.persist(storageLevel=StorageLevel.MEMORY_ONLY) # MEMORY_ONLY、MEMORY_ONLY_SER、DISK_ONLY 

  # 取消：从内存中移除，释放内存资源
  df.unpersist()
  ``` 
+ DataFrame操作: 聚合, 筛选, 排序, 连接等
  + 元素
    ``` python
    element = df.iloc[1, 2] # 二行三列
    element = df.at[3, "Name"] # name列第四行
    ``` 
  + 列
    ``` python
    df.Age 等效 df["age"]
    names_df = df.select("Name") # 选择列，创建一个新的DataFrame，name 不变
    renamed_df = df.select(df["Name"].alias("Full Name")) # 新的name

    df = df.withColumn("NewColumn", df["ExistingColumn"] * 2) # 创建新的列，值是existing 的两倍
    df = df.withColumn("Age", df["Age"] + 1) # 更新列中的值（创建新的，覆盖）

    new_df = df.select("Name", "Age").withColumn("Category", when(df.Age > 30, "Senior").otherwise("Junior")) # ，new_df 是一个新的DataFrame，它包含了原始DataFrame中的 "Name" 和 "Age" 列，并添加了一个名为 "Category" 的新列，该列的值根据 "Age" 列的条件而确定

    df = df.drop("ColumnToRemove") # 删除列
    ```
   + 聚合: 平均值、总和、最大值、最小值、计数
     + agg 基本用法：`df.agg({"columnName": "func 可自定义"})`
       + `avg`,`sum`,`min`,`count`
     + sum() avg() count()

        ``` python
        total_records = df.count() # 行数
        total_sales = df.select("Sales").sum().collect()[0][0]
        min_age = df.select("Age").min().collect()[0][0]


        df.agg({"Salary": "avg"})
        df.agg({"Salary": "avg", "Age": "max"}) # 多个聚合操作，以一次性计算多个统计信息
        
        avg_salary = df.select("Salary").agg({"Salary": "avg"}).first()[0]
        # average 只有一个元素，可以直接获取，first是第一行，[0]是第一个元素
        # 但是一般是在分布式计算环境中进行计算，将最终结果提取到本地，以避免内存和性能问题

        avg_salary = df.select("Salary").agg({"Salary": "avg"}).collect()[0][0] 
        # 1. agg 返回dataframe （在分布式节点上运算，数据分布在很多节点上，所以要获取数据，先collect到本地的数据结构中）
        # 2. collect() 返回一个列表，列表的元素是row对象 （直接理解成table就行了）
        ``` 
    + groupBy: 汇总，一半跟在聚合后
      + groupBy 按照指定的列进行分组，创建一个分组对象。可以在对分组对象使用agg计算统计值
      ``` python 
      grouped_df = df.groupBy("Category")

      # 聚合操作
      result_df = df.groupBy("Category").agg({"Value": "sum", "Quantity": "avg"}) # result dataframe 有三列： Category ｜ Value ｜ Quantity
      sum_df = df.groupBy("Category").sum("Value")
      avg_df = df.groupBy("Category").avg("Quantity")

      # 统计操作
      count_df = df.groupBy("Category").count() # dataframe: Category | count
      result = df.groupBy("ColumnToGroupBy").agg(count("*").alias("RecordCount")) # * 计算所有非空记录的数量

      # 筛选
      filtered_df = df.groupBy("Category").filter(df["Value"] > 100) # dataframe: Category | 
      ```

    + 筛选 
      + filter / where
      + 返回一个新的DataFrame
      + 逻辑运算符（ &  |）
      ```python
      filtered_df = df.filter(df.Age > 30)
      filtered_df = df.filter(df["ColumnName"] > 30)
      filtered_df = df.select("Name", "Age").filter(df.Age > 30)

      filtered_df = df.where(df["ColumnName"] > 30)
      
      filtered_df = df.filter((df["Age"] > 30) & (df["Gender"] == "Male")) # 筛选出年龄大于30且性别为"男"的行

      jun_29_operations = Bombing_Operations.where("MissionDate = '1966-06-29' AND TargetCountry='NORTH VIETNAM'") # sql 
      ``` 

    + 排序 
      + 生成新的DataFrame，包含所有列
      ``` python
      # orderBy
      sorted_df = df.orderBy("Age")
      sorted_df = df.orderBy(df["Column1"].asc(), df["Column2"].desc())   # 按照多个列进行排序，先按照列1升序，然后按照列2降序
      sorted_df = df.orderBy("ColumnName").select("ColumnName")  # 仅仅包含排序的列


      # sort
      sorted_df = df.sort(df["ColumnName"].desc())
      ``` 
    + 连接
      + inner: 返回都存在匹配的行
      + outer：返回所有的，不匹配填充null
      + left：以左侧为基准，返回右边匹配的，右边没有的返回null
      + right

      ``` python
      # format
      joined_df = df1.join(df2, on="common_column", how="xxxtype") 

      merged_df = df.select("Name", "Age").join(address_df, "Name")
      # df 中的 name age 形成新的dataframe
      # 新的frame 和 名为 address_df 的DataFrame进行连接。连接的条件是 "Name" 列，即两个DataFrame中的 "Name" 列必须具有相同的值才会进行连接
      ``` 
+ SQL查询
    ``` python
    df.createOrReplaceTempView("people")
    result = spark.sql("SELECT * FROM people WHERE Age > 30")
    ```
+ 保存数据
    ``` python
    df.write.csv("output.csv")
    df.write.mode('overwrite').json("df.json")
    ```
+ 停止SparkSession
    ```
    spark.stop()
    ```
## pandas % matplotlib
+ when the size of df is small, we can move to pandas and conduct visualization
``` python
pd = df.toPandas()
pd.head()

# bar chart
pl = missions_count_pd.plot(kind="bar",
                            x="Contr", y="MissionsCount",
                            figsize=(10, 7), log=True, alpha=0.5, color="olive") # alpha 透明度的参数
pl.set_xlabel("Country")
pl.set_ylabel("Number of Missions (Log scale)")
pl.set_title("Number of missions by Country")

# 针对聚合后的每一个组画图
fig = plt.figure(figsize=(10, 6))

# iterate the different groups to create a different series
for country, missions in missions_by_date.groupby("ContryFlyingMission"):
    plt.plot(missions["MissionDate"], missions["MissionsCount"], label=country)

plt.legend(loc='best')
```

## rdd
+ 创建rdd
    ``` python
    from pyspark import SparkContext

    sc = SparkContext("local", "MyRDDApp")  # 创建SparkContext
    data = [1, 2, 3, 4, 5]
    rdd = sc.parallelize(data)  # 从现有数据创建RDD. 将传递给它的数据集合分布到 Spark 集群中的多个节点上

    rdd.take(3) # 显现
    ```
+ 转换操作
  ``` python
  # 使用map将每个元素加倍
  rdd_doubled = rdd.map(lambda x: x * 2)

  # 使用filter筛选出偶数
  rdd_even = rdd.filter(lambda x: x % 2 == 0)

  # 使用reduce将所有元素相加
  total_sum = rdd.reduce(lambda x, y: x + y)
  ``` 
+ 持久化
  ``` python
  rdd.persist()

  # 将RDD持久化到内存
  rdd.persist(StorageLevel.MEMORY_ONLY)

  # 将RDD持久化到磁盘
  rdd.persist(StorageLevel.DISK_ONLY)
  ``` 
  + Example: 需要反复操作大型数据时使用
    ``` python
        # 创建大型RDD
        large_rdd = sc.parallelize(range(1, 1000000))

        # 将RDD持久化到磁盘
        large_rdd.persist(StorageLevel.DISK_ONLY)

        # 执行多个操作，多次重复使用数据
        sum_result = large_rdd.reduce(lambda x, y: x + y)
        avg_result = large_rdd.mean()
        # ...

    ``` 

+ 行动
  ```python
  # 获取RDD中的所有元素并返回为列表
  elements = rdd.collect()

  # 计算RDD中的元素数量
  count = rdd.count()

  # 获取第一个元素
  first_element = rdd.first()
  ``` 

+ 关闭
  ```python
  sc.stop()
  ```

## colab0 - Map Reduce
1. implicit - dataframe
   ``` python
   TakeoffLocationCounts = jun_29_operations\
                            .groupBy("TakeoffLocation").agg(count("*").alias("MissionsCount"))\
                            .sort(desc("MissionsCount"))
   ``` 
2. explicit - rdd
    ``` python
    # emit a pair
    all_locations = jun_29_operations.rdd.map(lambda row: (row.TakeoffLocation, 1)) # [('TAKHLI', 1), ('DANANG', 1), ('CONSTELLATION', 1)] 先从df->rdd, 然后把每行变成一个tuple (location, 1)
    locations_counts_rdd = all_locations.reduceByKey(lambda a, b: a+b).sortBy(lambda r: -r[1]) # [('CONSTELLATION', 87), ('TAKHLI', 56), ('KORAT', 55)]

    # rdd->df
    locations_counts_with_schema = locations_counts_rdd.map(lambda r: Row(TakeoffLocation=r[0], MissionsCount=r[1]))
    locations_counts = spark.createDataFrame(locations_counts_with_schema)
    ```

## colab1 - word count
``` python
txt = spark.read.text("pg100.txt") # 一列n行，每一行是一行文本
split_cols = split(lower(col('value')), " ") # 返回 df 中的 列对象
words = txt.select(explode(split_cols).alias("word")) # explode() 函数将数组中的每个元素拆分成多行; words df 中有一列 word

filtered_words = words.where("word != '' and word rlike '^[a-z]'")
first_letter = filtered_words.select(col("word").substr(1, 1).alias("first_letter"))
letter_counts = first_letter.groupBy("first_letter")\
                .agg(count("*").alias("Count"))\
                .sort(asc("first_letter"))

letter_counts.show(26)
```
